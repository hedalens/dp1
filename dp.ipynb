{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__zpj1SeqxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bz2\n",
        "import subprocess\n",
        "import re\n",
        "import xml.sax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhVvNvvGCAf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c5e7f203-b449-42de-ea2c-ab7e3361f8f8"
      },
      "source": [
        "pip install wikitextparser"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikitextparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/c5/2af0d7686e48072ed94fe568ce791166ac87d14c5bd0e0637fe095097f91/wikitextparser-0.43.2-py3-none-any.whl (63kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 20kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 30kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from wikitextparser) (0.2.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from wikitextparser) (2019.12.20)\n",
            "Installing collected packages: wikitextparser\n",
            "Successfully installed wikitextparser-0.43.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLKLKK2oCHqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikitextparser as wtp\n",
        "from wikitextparser import remove_markup"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UOwr5jSJ_8PT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b07da33c-703b-4ff8-9af9-f8fd2272e001"
      },
      "source": [
        "pip install mwparserfromhell"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mwparserfromhell\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/03/4fb04da533c7e237c0104151c028d8bff856293d34e51d208c529696fb79/mwparserfromhell-0.5.4.tar.gz (135kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 40kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 81kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 92kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 7.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mwparserfromhell\n",
            "  Building wheel for mwparserfromhell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mwparserfromhell: filename=mwparserfromhell-0.5.4-cp36-cp36m-linux_x86_64.whl size=183772 sha256=4c6202aaec953e9c8fb771cccd8f310ab3ec47d12ec72e9ccb580378d44dae48\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/76/d5/7088b941df3b362c45dd7912dd05314bc034751ec9cbca9a75\n",
            "Successfully built mwparserfromhell\n",
            "Installing collected packages: mwparserfromhell\n",
            "Successfully installed mwparserfromhell-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NqBfaNABXlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mwparserfromhell"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgoDI0sWetmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "4efbef0a-3361-4ebc-f5c1-9f1440c69287"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNXf0L4iewhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
        "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
        "    def __init__(self):\n",
        "        xml.sax.handler.ContentHandler.__init__(self)\n",
        "        self._buffer = None\n",
        "        self._values = {}\n",
        "        self._current_tag = None\n",
        "        self._pages = []\n",
        "\n",
        "    def characters(self, content):\n",
        "        \"\"\"Characters between opening and closing tags\"\"\"\n",
        "        if self._current_tag:\n",
        "            self._buffer.append(content)\n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        \"\"\"Opening tag of element\"\"\"\n",
        "        if name in ('title', 'text'):\n",
        "            self._current_tag = name\n",
        "            self._buffer = []\n",
        "\n",
        "    def endElement(self, name):\n",
        "        \"\"\"Closing tag of element\"\"\"\n",
        "        if name == self._current_tag:\n",
        "            self._values[name] = ' '.join(self._buffer)\n",
        "\n",
        "        if name == 'page':\n",
        "            self._pages.append((self._values['title'], self._values['text']))\n",
        "#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXlLVfj5e1Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Object for handling xml\n",
        "handler = WikiXmlHandler()# Parsing object\n",
        "parser = xml.sax.make_parser()\n",
        "parser.setContentHandler(handler)# Iteratively process file\n",
        "for line in subprocess.Popen(['bzcat'], \n",
        "                              stdin = open(\"/content/drive/My Drive/Colab Notebooks/etwiki-latest-pages-articles.xml.bz2\"), \n",
        "                              stdout = subprocess.PIPE).stdout:\n",
        "    parser.feed(line)\n",
        "#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAdg8avvGsI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_pages = dict(handler._pages)\n",
        "all_titles = list(all_pages)\n",
        "disambiguation_pages_texts = []\n",
        "for el in handler._pages:\n",
        "  is_disam_page = '{{täpsustus}}' in el[1] or '{{täpsustuslehekülg}}' in el[1] or '{{Täpsustus}}' in el[1] or '{{Täpsustuslehekülg}}' in el[1]\n",
        "  if (is_disam_page) and el[0] != 'Vikipeedia:Vormistusreeglid':\n",
        "    disambiguation_pages_texts.append(el)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Do_1sAie-jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make list of (disambiguation page, senses)-s\n",
        "disambiguation_pages_senses = []\n",
        "for el in disambiguation_pages_texts:\n",
        "  senses = []\n",
        "  v = el[1].split('Vaata ka')[0].split('*')\n",
        "  for s in v:\n",
        "    links = re.findall(\"\\[\\[(.*?)\\]\\]\",s)\n",
        "    if links != []:\n",
        "      senses.append(links[0].split('|')[0])\n",
        "  disambiguation_pages_senses.append((el[0],senses))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQ9SBFnyWgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "98790cb8-69f6-4016-e90c-994caa3eb663"
      },
      "source": [
        "pip install stanza"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (49.6.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv9J_KTIUEJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stanza"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1vxASQsSw8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stanza.download('et', verbose=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-sjALtTG2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "et_nlp = stanza.Pipeline('et', processors='tokenize,lemma', verbose=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb5sZ_FeAYEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing spaces so that parser can recognise elements\n",
        "def trim_unnessessary_spaces(text):\n",
        "\n",
        "  splitted = text.split(\"\\n\")\n",
        "  for i, item in enumerate(splitted):\n",
        "    splitted[i] = item.lstrip()\n",
        "    splitted[i] = re.sub('< ','<',splitted[i])\n",
        "    splitted[i] = re.sub(' >','>',splitted[i])\n",
        "    splitted[i] = re.sub('& nbsp','&nbsp',splitted[i])\n",
        "    splitted[i] = re.sub('& ndash','&ndash',splitted[i])     \n",
        "  text = '\\n'.join(splitted)\n",
        "  return text\n",
        "#function for cleaning text from markup and other, should return pure sentences in most cases\n",
        "def search(text):\n",
        "\n",
        "  regex = re.compile(r'==\\s*Vaata ka\\s*')\n",
        "  text = regex.split(text)[0]\n",
        "  regex = re.compile(r'==\\s*Kirjandus\\s*==')\n",
        "  text = regex.split(text)[0]\n",
        "  regex = re.compile(r'==\\s*Viited\\s*==')\n",
        "  text = regex.split(text)[0]\n",
        "  regex = re.compile(r'==\\s*Välislingid\\s*==')\n",
        "  text = regex.split(text)[0]\n",
        "  text = re.sub('\\[\\[.*:.*(\\|pisi|thumb|px).*\\]\\]','',text)\n",
        "\n",
        "  code = mwparserfromhell.parse(trim_unnessessary_spaces(text))\n",
        "\n",
        "  for tag in code.filter_tags(recursive=False):\n",
        "    if tag[0] == \"'\" and tag[-1]==\"'\":\n",
        "      code.replace(tag,tag.contents)\n",
        "    else:\n",
        "      code.replace(tag,\"\")\n",
        "\n",
        "  for argument in code.filter_arguments():\n",
        "    code.replace(argument, \"\")\n",
        "\n",
        "  for comment in code.filter_comments():\n",
        "    code.replace(comment,\"\")\n",
        "\n",
        "  for external_link in code.filter_external_links():\n",
        "    code.replace(external_link,\"\")\n",
        "\n",
        "  for heading in code.filter_headings():\n",
        "    code.replace(heading,\"\")\n",
        "\n",
        "  for html_entity in code.filter_html_entities():\n",
        "    code.replace(html_entity, \"\")\n",
        "\n",
        "  for template in code.filter_templates(recursive=False):\n",
        "    code.replace(template,\"\")\n",
        "\n",
        "  return remove_markup(str(code))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vtOnPHNzB7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ok(lsnc,lword):    #function used for checking if sentence contains word\n",
        "  \n",
        "  parts_o_word = len(lword)\n",
        "  if parts_o_word == 1:\n",
        "    return (lword[0] in lsnc) #generally checks if element of second list is in first list\n",
        "  else:                  #if multiple els in second list, eg word consists of 2 words, check if they are next to each other in 1st\n",
        "    for i,el in enumerate(lsnc):\n",
        "      if lsnc[i:i+parts_o_word] == lword:\n",
        "        return True\n",
        "  return False"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftX3j3gI1SY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#takes word, text, returns sentences containing the word\n",
        "def important_sentences(word,sentences_text):\n",
        "  head = et_nlp(word.split(' (')[0]).sentences\n",
        "  head = [[w.lemma for w in s.words] for s in head][0]\n",
        "  et_doc = et_nlp(sentences_text)\n",
        "  lst_of_snts = et_doc.sentences\n",
        "  lst_of_sntns_chosen = []\n",
        "  for sentence in lst_of_snts:\n",
        "    if ok([w.lemma for w in sentence.words], head):\n",
        "      lst_of_sntns_chosen.append(sentence.text)                \n",
        "  return lst_of_sntns_chosen"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6GB7DomCAhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fuction that takes sense and returns (sense, sentences containing that sense)\n",
        "def A(word, sense):\n",
        "  raw_text = all_pages[sense]\n",
        "  #cleaning the text from unnecessary elements\n",
        "  sentences_text = search(raw_text)\n",
        "  #excluding sentences that don't contain the word\n",
        "  important = important_sentences(word,sentences_text)\n",
        "  #if there is at least 1 sentence containing the WORD, return (sense, these sentences, number of these sentences)\n",
        "  if len(important) > 0:\n",
        "    length = len(important)\n",
        "    return (sense, important, length)\n",
        "  #if no article about that sense return 0\n",
        "  return 0"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GJgQtI-YAJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_list(initial):\n",
        "  titles_fixed = []\n",
        "  for sense in initial:\n",
        "    for title in all_titles:\n",
        "      if sense.lower() == title.lower():\n",
        "        titles_fixed.append(title)\n",
        "        break\n",
        "  redirect_fixed = []\n",
        "  for sense in titles_fixed:\n",
        "    text_of_page = all_pages[sense]\n",
        "    if '#suuna' in text_of_page or '#REDIRECT' in text_of_page or '#redirect' in text_of_page:\n",
        "      actual_page = re.findall('#[sunaredictREDICT]{5,8}:?\\s*\\[\\[(.*?)\\]\\]\\s*',text_of_page)[0].lower().strip().replace('_', ' ')\n",
        "      for title in all_titles:\n",
        "        if title.lower() == actual_page.lower():\n",
        "          redirect_fixed.append(title)\n",
        "          break\n",
        "    else:\n",
        "      redirect_fixed.append(sense)\n",
        "  return redirect_fixed"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN8T2MfrjZil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#function that takes a WORD and list of its senses and returns (WORD, output of function A for each sense)\n",
        "def B(word, senses):\n",
        "  senses = fix_list(senses)\n",
        "  value1 = word\n",
        "  value2 = []  #will contain results of function A on each sense eg (sense, clean text, number of sentences)\n",
        "  #filling up value2\n",
        "  for sense in senses:\n",
        "    sense_and_sntnces = A(word, sense)\n",
        "    if sense_and_sntnces != 0: #only if there was at least 2 senses with sentences about them\n",
        "      value2.append(sense_and_sntnces)\n",
        "  #accept only tuples where word has at least 2 senses which have sentences containing the word\n",
        "  if len(value2) < 2:\n",
        "    return 0\n",
        "  return (value1,value2)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo3hYH7POig3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_pages['Y'] = re.sub(\"'''Y''' on \\[\\[Saksamaa\\]\\]l '''''\\[\\[Bundeswehr\\]\\]'' < nowiki > 'i < /nowiki > ''' \\[\\[auto\\]\\]de tähis.\",\"\",all_pages['Y'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoiI4T0iNDBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filtering out relevant data\n",
        "nr_of_words = 0\n",
        "nr_of_senses = 0\n",
        "nr_of_sntnces = 0\n",
        "\n",
        "def senses_per_word_avg(nr_of_senses,nr_of_words):\n",
        "  return nr_of_senses / nr_of_words\n",
        "\n",
        "def sntnces_per_word_avg(nr_of_sntnces,nr_of_words):\n",
        "  return nr_of_sntnces / nr_of_words\n",
        "\n",
        "def sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses):\n",
        "  return nr_of_sntnces / nr_of_senses"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhigZqAINL-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/wiki_et.txt\",\"w\") as file1:\n",
        "  for el in disambiguation_pages_senses:\n",
        "    line = B(el[0],el[1])\n",
        "    if line != 0:\n",
        "      counter = 0\n",
        "      for element in line[1]:\n",
        "        counter += element[2]\n",
        "      if counter != len(line[1]):\n",
        "        nr_of_words += 1\n",
        "        nr_of_senses += len(line[1])\n",
        "        file1.write('WORD: '+line[0]+'\\n')\n",
        "        for i,elem in enumerate(line[1]):\n",
        "          nr_of_sntnces += elem[2]\n",
        "          file1.write('SENSE'+str(i)+': '+elem[0]+' - ')\n",
        "          file1.write(elem[1][0].strip('\\n')+'\\n')\n",
        "          file1.write('snc: ' + str(len(elem[1])) + '\\n')\n",
        "        file1.write('------------------------\\n')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bbEzFffYbOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e6e427b4-aa05-4e27-ed85-92a7c11f80fc"
      },
      "source": [
        "print(nr_of_words)\n",
        "print(nr_of_senses)\n",
        "print(nr_of_sntnces)\n",
        "print(round(senses_per_word_avg(nr_of_senses,nr_of_words),2))\n",
        "print(round(sntnces_per_word_avg(nr_of_sntnces,nr_of_words),2))\n",
        "print(round(sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses),2))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2608\n",
            "9025\n",
            "39211\n",
            "3.46\n",
            "15.03\n",
            "4.34\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}